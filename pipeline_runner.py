# -*- coding: utf-8 -*-
"""ETL practice

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17BQUrN86RSTeLR8GDYPy8yZA9L5fEA6E
"""

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta
import uuid

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

def generate_financial_dataset(num_rows=1000):
    """
    Generate a realistic financial trading dataset with various data quality issues
    Perfect for ETL practice
    """

    # Define reference data
    symbols = [
        'AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'META', 'NVDA', 'JPM', 'BAC', 'WFC',
        'SPY', 'QQQ', 'VTI', 'BTC-USD', 'ETH-USD', 'EUR/USD', 'GBP/USD', 'BOND_10Y',
        'GLD', 'SLV', 'XOM', 'CVX', 'JNJ', 'PFE', 'KO', 'PEP', 'DIS', 'NFLX', 'CRM', 'ORCL'
    ]

    security_names = {
        'AAPL': 'Apple Inc', 'GOOGL': 'Alphabet Inc Class A', 'MSFT': 'Microsoft Corporation',
        'TSLA': 'Tesla Inc', 'AMZN': 'Amazon.com Inc', 'META': 'Meta Platforms Inc',
        'NVDA': 'NVIDIA Corporation', 'JPM': 'JPMorgan Chase & Co', 'BAC': 'Bank of America Corp',
        'WFC': 'Wells Fargo & Co', 'SPY': 'SPDR S&P 500 ETF', 'QQQ': 'Invesco QQQ Trust',
        'VTI': 'Vanguard Total Stock Market ETF', 'BTC-USD': 'Bitcoin USD', 'ETH-USD': 'Ethereum USD',
        'EUR/USD': 'Euro US Dollar', 'GBP/USD': 'British Pound US Dollar', 'BOND_10Y': 'US Treasury 10 Year',
        'GLD': 'SPDR Gold Shares', 'SLV': 'iShares Silver Trust', 'XOM': 'Exxon Mobil Corp',
        'CVX': 'Chevron Corporation', 'JNJ': 'Johnson & Johnson', 'PFE': 'Pfizer Inc',
        'KO': 'The Coca-Cola Company', 'PEP': 'PepsiCo Inc', 'DIS': 'The Walt Disney Company',
        'NFLX': 'Netflix Inc', 'CRM': 'Salesforce Inc', 'ORCL': 'Oracle Corporation'
    }

    markets = {
        'AAPL': 'NASDAQ', 'GOOGL': 'NASDAQ', 'MSFT': 'NASDAQ', 'TSLA': 'NASDAQ',
        'AMZN': 'NASDAQ', 'META': 'NASDAQ', 'NVDA': 'NASDAQ', 'JPM': 'NYSE',
        'BAC': 'NYSE', 'WFC': 'NYSE', 'SPY': 'NYSE ARCA', 'QQQ': 'NASDAQ',
        'VTI': 'NYSE ARCA', 'BTC-USD': 'CRYPTO', 'ETH-USD': 'CRYPTO',
        'EUR/USD': 'FOREX', 'GBP/USD': 'FOREX', 'BOND_10Y': 'BOND',
        'GLD': 'NYSE ARCA', 'SLV': 'NYSE ARCA', 'XOM': 'NYSE', 'CVX': 'NYSE',
        'JNJ': 'NYSE', 'PFE': 'NYSE', 'KO': 'NYSE', 'PEP': 'NASDAQ',
        'DIS': 'NYSE', 'NFLX': 'NASDAQ', 'CRM': 'NYSE', 'ORCL': 'NYSE'
    }

    transaction_types = ['BUY', 'SELL', 'DIVIDEND']
    account_types = ['Individual', 'Retirement', 'Corporate', 'Trust', 'Joint']
    broker_ids = ['BRK001', 'BRK002', 'BRK003', 'BRK004', 'BRK005']
    risk_ratings = ['Low', 'Medium', 'High', 'Very High', '', 'MEDIUM', 'low', 'HIGH', 'Med']

    # Price ranges for different securities (min, max)
    price_ranges = {
        'AAPL': (150, 200), 'GOOGL': (120, 180), 'MSFT': (300, 420), 'TSLA': (180, 300),
        'AMZN': (140, 180), 'META': (200, 350), 'NVDA': (400, 900), 'JPM': (140, 180),
        'BAC': (25, 45), 'WFC': (35, 55), 'SPY': (400, 500), 'QQQ': (350, 450),
        'VTI': (200, 250), 'BTC-USD': (35000, 70000), 'ETH-USD': (2000, 4000),
        'EUR/USD': (1.05, 1.12), 'GBP/USD': (1.25, 1.35), 'BOND_10Y': (95, 105),
        'GLD': (180, 220), 'SLV': (20, 30), 'XOM': (95, 120), 'CVX': (140, 170),
        'JNJ': (150, 170), 'PFE': (25, 35), 'KO': (55, 65), 'PEP': (160, 180),
        'DIS': (85, 115), 'NFLX': (350, 500), 'CRM': (200, 280), 'ORCL': (100, 130)
    }

    notes_options = [
        'Long-term investment', 'Short-term trade', 'Rebalancing', 'Profit taking',
        'Dollar cost averaging', 'Tax loss harvesting', 'Dividend reinvestment',
        'Portfolio diversification', 'Risk management', 'Market timing',
        'Quarterly rebalancing', 'Year-end selling', 'Estate planning', ''
    ]

    # Date formatting functions (introducing inconsistencies)
    def format_date_randomly(date_obj):
        """Apply random date formatting to introduce data quality issues"""
        formats = [
            lambda d: d.strftime('%Y-%m-%d'),           # YYYY-MM-DD
            lambda d: d.strftime('%m/%d/%Y'),           # MM/DD/YYYY
            lambda d: d.strftime('%d/%m/%Y'),           # DD/MM/YYYY
            lambda d: d.strftime('%m-%d-%Y'),           # MM-DD-YYYY
            lambda d: d.strftime('%d-%m-%Y'),           # DD-MM-YYYY
            lambda d: d.strftime('%d-%b-%Y'),           # DD-Mon-YYYY
            lambda d: d.strftime('%Y.%m.%d'),           # YYYY.MM.DD
            lambda d: d.strftime('%d %B %Y'),           # DD Month YYYY
        ]
        return random.choice(formats)(date_obj)

    def generate_random_date():
        """Generate random date in 2024"""
        start_date = datetime(2024, 1, 1)
        end_date = datetime(2024, 12, 31)
        time_between = end_date - start_date
        days_between = time_between.days
        random_days = random.randrange(days_between)
        return start_date + timedelta(days=random_days)

    def get_settlement_date(trade_date, transaction_type):
        """Generate settlement date (usually 1-3 business days later, sometimes missing)"""
        if transaction_type == 'DIVIDEND' or random.random() < 0.08:
            return ''  # 8% missing settlement dates

        settlement = trade_date + timedelta(days=random.randint(1, 3))
        return format_date_randomly(settlement)

    def format_price_randomly(price):
        """Apply random price formatting to introduce inconsistencies"""
        formats = [
            lambda p: f"${p:.2f}",                      # $175.50
            lambda p: f"{p:.2f}",                       # 175.50
            lambda p: f"${p:.4f}",                      # $175.5000
            lambda p: f"{p:.6f}".rstrip('0').rstrip('.'), # 175.5 (variable decimals)
            lambda p: f"{p:.3f}",                       # 175.500
        ]
        return random.choice(formats)(price)

    def format_amount_randomly(amount):
        """Apply random amount formatting"""
        formats = [
            lambda a: f"${a:.2f}",                      # $-17550.00
            lambda a: f"{a:.2f}",                       # -17550.00
            lambda a: f"${a:,.2f}",                     # $-17,550.00
            lambda a: f"{a:,.2f}",                      # -17,550.00
        ]
        return random.choice(formats)(amount)

    # Generate the dataset
    data = []

    for i in range(1, num_rows + 1):
        # Basic transaction info
        transaction_id = f"TXN{i:06d}"
        account_number = f"ACC-{random.randint(100000, 999999)}"
        customer_id = f"CUST_{random.randint(1, 1000)}"

        # Dates
        trade_date = generate_random_date()
        trade_date_formatted = format_date_randomly(trade_date)

        # Security and transaction details
        symbol = random.choice(symbols)
        security_name = security_names[symbol]
        transaction_type = random.choice(transaction_types)
        market = markets[symbol]

        # Settlement date
        settlement_date = get_settlement_date(trade_date, transaction_type)

        # Price and quantity
        price_range = price_ranges.get(symbol, (50, 200))
        price = random.uniform(price_range[0], price_range[1])

        # Generate quantity based on asset type
        if transaction_type == 'DIVIDEND':
            quantity = random.randint(50, 1000)
            price_per_share = random.uniform(0.10, 5.0)  # Dividend per share
        elif 'BTC' in symbol or 'ETH' in symbol:
            quantity = random.uniform(0.001, 10.0)  # Crypto fractions
            price_per_share = price
        elif '/' in symbol:  # Forex
            quantity = random.randint(10000, 500000)  # Large forex amounts
            price_per_share = price
        else:  # Regular stocks/ETFs
            quantity = random.randint(1, 500)
            price_per_share = price

        # Calculate amounts
        if transaction_type == 'DIVIDEND':
            gross_amount = quantity * price_per_share
        else:
            gross_amount = quantity * price_per_share
            if transaction_type == 'BUY':
                gross_amount = -gross_amount  # Negative for purchases

        # Commission and tax
        commission = random.uniform(5.0, 55.0)
        tax_amount = 0.0
        if transaction_type == 'SELL' and random.random() > 0.1:  # 10% missing tax
            tax_amount = abs(gross_amount) * random.uniform(0.005, 0.02)  # 0.5-2% tax

        # Account and broker info
        account_type = random.choice(account_types)
        broker_id = random.choice(broker_ids)
        risk_rating = random.choice(risk_ratings)

        # Timestamp (add random time to trade date)
        timestamp = trade_date + timedelta(
            hours=random.randint(9, 16),
            minutes=random.randint(0, 59),
            seconds=random.randint(0, 59)
        )

        # Notes (30% chance of being empty)
        client_notes = '' if random.random() < 0.3 else random.choice(notes_options)

        # Create the record with formatting inconsistencies
        record = {
            'transaction_id': transaction_id,
            'account_number': account_number,
            'customer_id': customer_id,
            'trade_date': trade_date_formatted,
            'settlement_date': settlement_date,
            'symbol': symbol,
            'security_name': security_name,
            'transaction_type': transaction_type,
            'quantity': f"{quantity:.6f}".rstrip('0').rstrip('.'),  # Remove trailing zeros
            'price_per_share': format_price_randomly(price_per_share),
            'total_amount': format_amount_randomly(gross_amount),
            'currency': 'USD',
            'commission': f"{commission:.2f}",
            'tax': '' if tax_amount == 0 and random.random() < 0.1 else f"{tax_amount:.2f}",
            'account_type': account_type,
            'broker_id': broker_id,
            'market': market,
            'timestamp': timestamp.isoformat() + 'Z',
            'client_notes': client_notes,
            'risk_rating': risk_rating
        }

        data.append(record)

    return pd.DataFrame(data)

# Generate the dataset
print("üöÄ Generating 1000-row financial dataset...")
df = generate_financial_dataset(1000)

print(f"‚úÖ Dataset generated successfully!")
print(f"üìä Shape: {df.shape}")
print(f"üìã Columns: {list(df.columns)}")
print("\nüîç First 5 rows:")
print(df.head())

print("\nüìà Data Quality Issues Summary:")
print(f"Missing settlement_dates: {df['settlement_date'].eq('').sum()}")
print(f"Missing tax amounts: {df['tax'].eq('').sum()}")
print(f"Missing risk_ratings: {df['risk_rating'].eq('').sum()}")
print(f"Missing client_notes: {df['client_notes'].eq('').sum()}")

print("\nüíæ Saving to CSV...")
df.to_csv('financial_transactions_1000_rows.csv', index=False)
print("‚úÖ Saved as 'financial_transactions_1000_rows.csv'")

print("\nüéØ Dataset is ready for ETL practice!")
print("This dataset includes:")
print("- Multiple date formats (7 different formats)")
print("- Currency formatting inconsistencies")
print("- Missing values (5-10% across different fields)")
print("- Text case variations")
print("- Decimal precision inconsistencies")
print("- Real-world financial data complexity")

# Display sample of data quality issues
print("\nüîß Sample Data Quality Issues Found:")
print("\nDate formats variety:")
print(df['trade_date'].head(10).tolist())

print("\nPrice formatting variety:")
print(df['price_per_share'].head(10).tolist())

print("\nRisk rating inconsistencies:")
print(df['risk_rating'].value_counts())

print("\nüìö Next Steps for ETL Practice:")
print("1. Load this CSV into your ETL tool")
print("2. Profile the data to understand quality issues")
print("3. Implement the transformation rules from the schema")
print("4. Build the star schema with fact and dimension tables")
print("5. Apply data quality validations")

"""FACT TABLE (transactions_fact):
transaction_sk | customer_sk | security_sk | amount | date_sk

DIMENSION TABLES:
- customer_dimension: customer_sk | customer_id | risk_rating
- security_dimension: security_sk | symbol | security_name  
- date_dimension: date_sk | date | year | quarter | month

3. Progressive ETL Steps

Why not fix everything at once?





Our Approach:

Extract: Get data as-is, understand problems

Profile: Quantify data quality issues

Clean: Fix formatting, handle missing values

Transform: Apply business rules, calculations

Model: Create star schema structure

Load: Insert into target tables

Validate: Ensure data quality in final result
"""

# =====================================================
# ETL STEP 1: DATA PROFILING
# Understanding Data Quality Before Transformation
# =====================================================

import pandas as pd
import numpy as np
import re
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

print("üîç ETL STEP 1: DATA PROFILING")
print("=" * 50)
print("Goal: Understand data quality issues before cleaning")
print("=" * 50)

# Load the data
df = pd.read_csv('financial_transactions_1000_rows.csv')
print(f"‚úÖ Loaded {len(df):,} rows and {len(df.columns)} columns")

# =====================================================
# 1. BASIC DATASET OVERVIEW
# =====================================================

def basic_overview(df):
    """Get basic information about the dataset"""
    print("\nüìä BASIC DATASET OVERVIEW")
    print("-" * 30)

    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
    print(f"Duplicate rows: {df.duplicated().sum()}")

    print(f"\nüìã Data Types:")
    dtype_counts = df.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"  {dtype}: {count} columns")

    return df.shape, df.duplicated().sum()

basic_info = basic_overview(df)

# =====================================================
# 2. MISSING VALUES ANALYSIS
# =====================================================

def analyze_missing_values(df):
    """Comprehensive missing values analysis"""
    print("\nüï≥Ô∏è  MISSING VALUES ANALYSIS")
    print("-" * 30)

    # Calculate missing values
    missing_stats = pd.DataFrame({
        'Column': df.columns,
        'Missing_Count': df.isnull().sum(),
        'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),
        'Data_Type': df.dtypes,
        'Non_Null_Count': df.count()
    })

    # Include empty strings as missing for string columns
    for col in df.select_dtypes(include=['object']).columns:
        empty_strings = (df[col] == '').sum()
        missing_stats.loc[missing_stats['Column'] == col, 'Missing_Count'] += empty_strings
        missing_stats.loc[missing_stats['Column'] == col, 'Missing_Percentage'] = (
            missing_stats.loc[missing_stats['Column'] == col, 'Missing_Count'] / len(df) * 100
        ).round(2)

    # Sort by missing percentage
    missing_stats = missing_stats.sort_values('Missing_Percentage', ascending=False)

    print("Missing Values Summary:")
    print(missing_stats[missing_stats['Missing_Count'] > 0])

    if missing_stats['Missing_Count'].sum() == 0:
        print("‚úÖ No missing values found!")
    else:
        print(f"\nüö® Total missing values: {missing_stats['Missing_Count'].sum():,}")
        print(f"üìä Columns with missing data: {(missing_stats['Missing_Count'] > 0).sum()}")

    return missing_stats

missing_analysis = analyze_missing_values(df)

# =====================================================
# 3. DATE FORMATS ANALYSIS
# =====================================================

def analyze_date_formats(df, date_columns):
    """Analyze different date formats in the dataset"""
    print("\nüìÖ DATE FORMATS ANALYSIS")
    print("-" * 30)

    date_patterns = {}

    for col in date_columns:
        if col in df.columns:
            print(f"\nüîç Analyzing column: {col}")

            # Get non-empty values
            non_empty = df[col].dropna()
            non_empty = non_empty[non_empty != '']

            if len(non_empty) == 0:
                print("  ‚ö†Ô∏è  No non-empty values found")
                continue

            # Sample of unique formats
            unique_formats = non_empty.unique()[:10]  # Show first 10 unique values
            print(f"  üìù Sample values ({len(non_empty)} total, {len(df[col].unique())} unique):")
            for i, fmt in enumerate(unique_formats, 1):
                print(f"    {i}. '{fmt}'")

            # Try to identify patterns
            patterns = []
            for value in non_empty.head(50):  # Check first 50 values
                value_str = str(value)
                if re.match(r'\d{4}-\d{2}-\d{2}', value_str):
                    patterns.append('YYYY-MM-DD')
                elif re.match(r'\d{2}/\d{2}/\d{4}', value_str):
                    patterns.append('MM/DD/YYYY or DD/MM/YYYY')
                elif re.match(r'\d{2}-\d{2}-\d{4}', value_str):
                    patterns.append('MM-DD-YYYY or DD-MM-YYYY')
                elif re.match(r'\d{1,2}-\w{3}-\d{4}', value_str):
                    patterns.append('DD-Mon-YYYY')
                elif re.match(r'\d{4}\.\d{2}\.\d{2}', value_str):
                    patterns.append('YYYY.MM.DD')
                else:
                    patterns.append('Unknown/Other')

            pattern_counts = Counter(patterns)
            print(f"  üéØ Detected patterns:")
            for pattern, count in pattern_counts.most_common():
                percentage = (count / len(patterns)) * 100
                print(f"    {pattern}: {count} ({percentage:.1f}%)")

            date_patterns[col] = pattern_counts

    return date_patterns

# Analyze date columns
date_columns = ['trade_date', 'settlement_date']
date_analysis = analyze_date_formats(df, date_columns)

# =====================================================
# 4. CURRENCY/NUMERIC FORMATS ANALYSIS
# =====================================================

def analyze_numeric_formats(df, numeric_columns):
    """Analyze numeric/currency formatting issues"""
    print("\nüí∞ NUMERIC/CURRENCY FORMATS ANALYSIS")
    print("-" * 40)

    numeric_issues = {}

    for col in numeric_columns:
        if col in df.columns:
            print(f"\nüîç Analyzing column: {col}")

            # Get sample values
            sample_values = df[col].dropna().head(15)
            print(f"  üìù Sample values:")
            for i, val in enumerate(sample_values, 1):
                print(f"    {i}. '{val}' (type: {type(val).__name__})")

            # Analyze patterns
            unique_vals = df[col].dropna().astype(str).unique()[:20]

            patterns = {
                'has_dollar_sign': 0,
                'no_dollar_sign': 0,
                'negative_values': 0,
                'decimal_places': [],
                'contains_commas': 0
            }

            for val in unique_vals:
                val_str = str(val)
                if '$' in val_str:
                    patterns['has_dollar_sign'] += 1
                else:
                    patterns['no_dollar_sign'] += 1

                if '-' in val_str:
                    patterns['negative_values'] += 1

                if ',' in val_str:
                    patterns['contains_commas'] += 1

                # Count decimal places
                if '.' in val_str:
                    decimal_part = val_str.split('.')[-1]
                    # Remove non-numeric characters from end
                    decimal_clean = re.sub(r'[^\d]', '', decimal_part)
                    if decimal_clean:
                        patterns['decimal_places'].append(len(decimal_clean))

            print(f"  üéØ Format patterns:")
            print(f"    With $ sign: {patterns['has_dollar_sign']}")
            print(f"    Without $ sign: {patterns['no_dollar_sign']}")
            print(f"    Negative values: {patterns['negative_values']}")
            print(f"    With commas: {patterns['contains_commas']}")

            if patterns['decimal_places']:
                decimal_counter = Counter(patterns['decimal_places'])
                print(f"    Decimal places distribution:")
                for places, count in decimal_counter.most_common():
                    print(f"      {places} decimals: {count} values")

            numeric_issues[col] = patterns

    return numeric_issues

# Analyze numeric columns
numeric_columns = ['price_per_share', 'total_amount', 'commission', 'tax', 'quantity']
numeric_analysis = analyze_numeric_formats(df, numeric_columns)

# =====================================================
# 5. TEXT DATA ANALYSIS
# =====================================================

def analyze_text_data(df, text_columns):
    """Analyze text data for inconsistencies"""
    print("\nüìù TEXT DATA ANALYSIS")
    print("-" * 25)

    text_issues = {}

    for col in text_columns:
        if col in df.columns:
            print(f"\nüîç Analyzing column: {col}")

            # Value counts
            value_counts = df[col].value_counts(dropna=False)
            print(f"  üìä Unique values: {len(value_counts)}")
            print(f"  üìù Top values:")

            for val, count in value_counts.head(10).items():
                percentage = (count / len(df)) * 100
                print(f"    '{val}': {count} ({percentage:.1f}%)")

            # Check for case inconsistencies
            if col in ['risk_rating', 'account_type', 'transaction_type']:
                non_empty = df[col].dropna()
                non_empty = non_empty[non_empty != '']

                if len(non_empty) > 0:
                    # Check for same values with different cases
                    lower_vals = non_empty.str.lower().value_counts()
                    original_vals = non_empty.value_counts()

                    if len(lower_vals) < len(original_vals):
                        print(f"  ‚ö†Ô∏è  Case inconsistencies detected!")
                        print(f"    Unique values (case-sensitive): {len(original_vals)}")
                        print(f"    Unique values (case-insensitive): {len(lower_vals)}")

            text_issues[col] = {
                'unique_count': len(value_counts),
                'null_count': value_counts.get(np.nan, 0) + value_counts.get('', 0),
                'top_values': value_counts.head(5).to_dict()
            }

    return text_issues

# Analyze text columns
text_columns = ['transaction_type', 'account_type', 'broker_id', 'market', 'risk_rating', 'client_notes']
text_analysis = analyze_text_data(df, text_columns)

# =====================================================
# 6. BUSINESS LOGIC VALIDATION
# =====================================================

def validate_business_logic(df):
    """Check for business logic violations"""
    print("\nüè¶ BUSINESS LOGIC VALIDATION")
    print("-" * 35)

    violations = {}

    # 1. Check transaction amounts vs transaction type
    print("üîç Checking transaction amount logic...")

    # Convert amounts to numeric for analysis (basic conversion)
    df_temp = df.copy()

    # Try to extract numeric values from total_amount
    df_temp['amount_numeric'] = df_temp['total_amount'].astype(str).str.replace('$', '').str.replace(',', '')
    try:
        df_temp['amount_numeric'] = pd.to_numeric(df_temp['amount_numeric'], errors='coerce')

        # Business rule: BUY transactions should be negative, SELL should be positive
        buy_positive = df_temp[(df_temp['transaction_type'] == 'BUY') & (df_temp['amount_numeric'] > 0)]
        sell_negative = df_temp[(df_temp['transaction_type'] == 'SELL') & (df_temp['amount_numeric'] < 0)]

        print(f"  ‚ö†Ô∏è  BUY transactions with positive amounts: {len(buy_positive)}")
        print(f"  ‚ö†Ô∏è  SELL transactions with negative amounts: {len(sell_negative)}")

        violations['amount_logic'] = {
            'buy_positive': len(buy_positive),
            'sell_negative': len(sell_negative)
        }
    except:
        print("  ‚ùå Could not validate amount logic due to format issues")

    # 2. Check for duplicate transaction IDs
    print(f"\nüîç Checking for duplicate transaction IDs...")
    duplicate_txns = df['transaction_id'].duplicated().sum()
    print(f"  Duplicate transaction IDs: {duplicate_txns}")
    violations['duplicate_transactions'] = duplicate_txns

    # 3. Check settlement dates vs trade dates (when both exist)
    print(f"\nüîç Checking settlement vs trade date logic...")
    # This would require date parsing, which we'll do in the cleaning step
    print("  üìù Will validate after date cleaning step")

    return violations

business_validation = validate_business_logic(df)

# =====================================================
# 7. DATA PROFILING SUMMARY
# =====================================================

def generate_profiling_summary(df, missing_stats, date_patterns, numeric_issues, text_issues, business_violations):
    """Generate a comprehensive data profiling summary"""
    print("\n" + "="*60)
    print("üìã DATA PROFILING SUMMARY REPORT")
    print("="*60)

    print(f"\nüìä DATASET OVERVIEW:")
    print(f"  ‚Ä¢ Total Records: {len(df):,}")
    print(f"  ‚Ä¢ Total Columns: {len(df.columns)}")
    print(f"  ‚Ä¢ Duplicate Rows: {df.duplicated().sum()}")

    print(f"\nüï≥Ô∏è  DATA QUALITY ISSUES:")
    total_missing = missing_stats['Missing_Count'].sum()
    if total_missing > 0:
        print(f"  ‚Ä¢ Missing Values: {total_missing:,} ({(total_missing/(len(df)*len(df.columns))*100):.1f}% of all cells)")
        print(f"  ‚Ä¢ Columns with Missing Data: {(missing_stats['Missing_Count'] > 0).sum()}")
    else:
        print(f"  ‚Ä¢ Missing Values: None detected ‚úÖ")

    print(f"\nüìÖ DATE FORMAT ISSUES:")
    for col, patterns in date_patterns.items():
        if patterns:
            print(f"  ‚Ä¢ {col}: {len(patterns)} different formats detected")
        else:
            print(f"  ‚Ä¢ {col}: No data to analyze")

    print(f"\nüí∞ NUMERIC FORMAT ISSUES:")
    for col, issues in numeric_issues.items():
        dollar_inconsistency = issues['has_dollar_sign'] > 0 and issues['no_dollar_sign'] > 0
        decimal_variety = len(set(issues['decimal_places'])) > 1 if issues['decimal_places'] else False

        if dollar_inconsistency or decimal_variety:
            print(f"  ‚Ä¢ {col}: Format inconsistencies detected")
        else:
            print(f"  ‚Ä¢ {col}: Consistent formatting ‚úÖ")

    print(f"\nüìù TEXT DATA ISSUES:")
    for col, issues in text_issues.items():
        if col in ['risk_rating', 'account_type'] and issues['unique_count'] > 5:
            print(f"  ‚Ä¢ {col}: Potential case inconsistencies")
        elif issues['null_count'] > 0:
            print(f"  ‚Ä¢ {col}: {issues['null_count']} missing values")

    print(f"\nüè¶ BUSINESS LOGIC VIOLATIONS:")
    for violation_type, count in business_violations.items():
        if isinstance(count, dict):
            for sub_type, sub_count in count.items():
                if sub_count > 0:
                    print(f"  ‚Ä¢ {violation_type}.{sub_type}: {sub_count} violations")
        elif count > 0:
            print(f"  ‚Ä¢ {violation_type}: {count} violations")

    print(f"\nüéØ RECOMMENDED NEXT STEPS:")
    print(f"  1. üßπ Clean date formats ‚Üí standardize to YYYY-MM-DD")
    print(f"  2. üí∞ Clean numeric formats ‚Üí remove $, standardize decimals")
    print(f"  3. üìù Standardize text ‚Üí consistent case, handle missing values")
    print(f"  4. ‚úÖ Validate business rules ‚Üí fix amount/type mismatches")
    print(f"  5. üèóÔ∏è  Build star schema ‚Üí create fact and dimension tables")

    print("\n" + "="*60)

# Generate final summary
generate_profiling_summary(df, missing_analysis, date_analysis, numeric_analysis, text_analysis, business_validation)

print(f"\n‚úÖ DATA PROFILING COMPLETE!")
print(f"üìã You now understand your data quality issues")
print(f"üöÄ Ready for Step 2: Data Cleaning")

"""üîç What This Profiling Code Does:
1. Basic Dataset Overview

Dataset shape, memory usage, duplicates
Data type distribution

2. Missing Values Analysis

Counts both NULL and empty strings
Shows percentage of missing data per column
Identifies which columns need attention

3. Date Format Analysis

Detects different date patterns in your data
Shows examples of each format
Quantifies the variety of formats

4. Numeric Format Analysis

Identifies currency formatting inconsistencies ($175.50 vs 175.50)
Counts decimal place variations
Detects commas, negative values

5. Text Data Analysis

Shows unique values and their frequencies
Detects case inconsistencies (Low vs low vs LOW)
Identifies potential standardization needs

6. Business Logic Validation

Checks if BUY transactions are negative (as they should be)
Validates transaction ID uniqueness
Prepares for date relationship validation
"""

# =====================================================
# ETL STEP 2: DATA CLEANING
# Systematic Data Quality Fixes
# =====================================================

import pandas as pd
import numpy as np
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("üßπ ETL STEP 2: DATA CLEANING")
print("=" * 40)
print("Goal: Fix all data quality issues identified in profiling")
print("=" * 40)

# Load the raw data
df_raw = pd.read_csv('financial_transactions_1000_rows.csv')
print(f"‚úÖ Loaded {len(df_raw):,} rows for cleaning")

# Create a copy for cleaning (keep raw data untouched)
df_clean = df_raw.copy()
print(f"üìã Created working copy for transformations")

# =====================================================
# 1. DATE STANDARDIZATION
# =====================================================

def clean_dates(df, date_columns):
    """Standardize all date formats to YYYY-MM-DD"""
    print(f"\nüìÖ CLEANING DATE FORMATS")
    print("-" * 30)

    cleaning_stats = {}

    for col in date_columns:
        if col not in df.columns:
            continue

        print(f"\nüîç Processing column: {col}")
        original_count = df[col].notna().sum()

        # Store original values for comparison
        df[f'{col}_original'] = df[col].copy()

        # Handle empty strings and nulls
        df[col] = df[col].replace('', np.nan)

        # Function to parse various date formats
        def parse_flexible_date(date_str):
            if pd.isna(date_str) or date_str == '':
                return np.nan

            date_str = str(date_str).strip()

            # List of possible date formats to try
            date_formats = [
                '%Y-%m-%d',         # 2024-01-15
                '%m/%d/%Y',         # 01/15/2024
                '%d/%m/%Y',         # 15/01/2024
                '%m-%d-%Y',         # 01-15-2024
                '%d-%m-%Y',         # 15-01-2024
                '%d-%b-%Y',         # 15-Jan-2024
                '%Y.%m.%d',         # 2024.01.15
                '%d %B %Y',         # 15 January 2024
                '%B %d %Y',         # January 15 2024
                '%d %b %Y',         # 15 Jan 2024
                '%Y/%m/%d',         # 2024/01/15
                '%d.%m.%Y',         # 15.01.2024
            ]

            # Try each format
            for fmt in date_formats:
                try:
                    parsed_date = datetime.strptime(date_str, fmt)
                    return parsed_date.strftime('%Y-%m-%d')
                except ValueError:
                    continue

            # If no format works, try pandas parser as last resort
            try:
                parsed_date = pd.to_datetime(date_str, dayfirst=False)
                return parsed_date.strftime('%Y-%m-%d')
            except:
                print(f"    ‚ö†Ô∏è  Could not parse: '{date_str}'")
                return np.nan

        # Apply the cleaning function
        df[col] = df[col].apply(parse_flexible_date)

        # Count results
        cleaned_count = df[col].notna().sum()
        failed_count = original_count - cleaned_count

        print(f"  üìä Results:")
        print(f"    Original non-empty: {original_count}")
        print(f"    Successfully cleaned: {cleaned_count}")
        print(f"    Failed to parse: {failed_count}")
        print(f"    Success rate: {(cleaned_count/original_count*100):.1f}%" if original_count > 0 else "N/A")

        # Show examples
        sample_before = df[f'{col}_original'].dropna().head(3).tolist()
        sample_after = df[col].dropna().head(3).tolist()

        print(f"  üìù Examples:")
        for i, (before, after) in enumerate(zip(sample_before, sample_after)):
            print(f"    {i+1}. '{before}' ‚Üí '{after}'")

        cleaning_stats[col] = {
            'original_count': original_count,
            'cleaned_count': cleaned_count,
            'failed_count': failed_count
        }

    return df, cleaning_stats

# Clean date columns
date_columns = ['trade_date', 'settlement_date']
df_clean, date_stats = clean_dates(df_clean, date_columns)

# =====================================================
# 2. NUMERIC DATA CLEANING
# =====================================================

def clean_numeric_data(df, numeric_columns):
    """Clean and standardize numeric/currency data"""
    print(f"\nüí∞ CLEANING NUMERIC DATA")
    print("-" * 30)

    cleaning_stats = {}

    for col in numeric_columns:
        if col not in df.columns:
            continue

        print(f"\nüîç Processing column: {col}")

        # Store original for comparison
        df[f'{col}_original'] = df[col].copy()

        # Function to clean numeric values
        def clean_numeric_value(value):
            if pd.isna(value) or value == '':
                return np.nan

            # Convert to string for processing
            value_str = str(value).strip()

            # Remove currency symbols and commas
            cleaned = value_str.replace('$', '').replace(',', '')

            # Handle negative values
            is_negative = '-' in cleaned or cleaned.startswith('(')
            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')

            try:
                # Convert to float
                numeric_value = float(cleaned)

                # Apply negative sign if needed
                if is_negative:
                    numeric_value = -numeric_value

                return numeric_value
            except ValueError:
                print(f"    ‚ö†Ô∏è  Could not convert: '{value_str}'")
                return np.nan

        # Apply cleaning
        original_count = df[col].notna().sum()
        df[col] = df[col].apply(clean_numeric_value)
        cleaned_count = df[col].notna().sum()

        print(f"  üìä Results:")
        print(f"    Original non-empty: {original_count}")
        print(f"    Successfully cleaned: {cleaned_count}")
        print(f"    Failed conversions: {original_count - cleaned_count}")

        # Show examples
        sample_original = df[f'{col}_original'].dropna().head(3)
        sample_cleaned = df[col].dropna().head(3)

        print(f"  üìù Examples:")
        for i, (orig_idx, cleaned_idx) in enumerate(zip(sample_original.index, sample_cleaned.index)):
            if orig_idx == cleaned_idx:  # Same row
                orig_val = sample_original.loc[orig_idx]
                clean_val = sample_cleaned.loc[cleaned_idx]
                print(f"    {i+1}. '{orig_val}' ‚Üí {clean_val}")

        cleaning_stats[col] = {
            'original_count': original_count,
            'cleaned_count': cleaned_count,
            'failed_count': original_count - cleaned_count
        }

    return df, cleaning_stats

# Clean numeric columns
numeric_columns = ['price_per_share', 'total_amount', 'commission', 'tax', 'quantity']
df_clean, numeric_stats = clean_numeric_data(df_clean, numeric_columns)

# =====================================================
# 3. TEXT DATA STANDARDIZATION
# =====================================================

def clean_text_data(df):
    """Standardize text data for consistency"""
    print(f"\nüìù CLEANING TEXT DATA")
    print("-" * 25)

    cleaning_stats = {}

    # Define standardization rules
    standardization_rules = {
        'risk_rating': {
            'low': 'Low',
            'medium': 'Medium',
            'med': 'Medium',
            'high': 'High',
            'very high': 'Very High',
            '': 'Unknown'
        },
        'account_type': {
            'individual': 'Individual',
            'retirement': 'Retirement',
            'corporate': 'Corporate',
            'trust': 'Trust',
            'joint': 'Joint'
        },
        'transaction_type': {
            'buy': 'BUY',
            'sell': 'SELL',
            'dividend': 'DIVIDEND'
        }
    }

    for col, rules in standardization_rules.items():
        if col not in df.columns:
            continue

        print(f"\nüîç Processing column: {col}")

        # Store original
        df[f'{col}_original'] = df[col].copy()

        # Get value counts before cleaning
        before_counts = df[col].value_counts(dropna=False)
        print(f"  üìä Before cleaning - unique values: {len(before_counts)}")

        # Clean the data
        def standardize_text(value):
            if pd.isna(value):
                return 'Unknown'

            value_str = str(value).strip().lower()

            # Apply standardization rules
            for key, standard_value in rules.items():
                if value_str == key:
                    return standard_value

            # If no rule matches, title case the original
            return str(value).strip().title() if str(value).strip() else 'Unknown'

        df[col] = df[col].apply(standardize_text)

        # Get value counts after cleaning
        after_counts = df[col].value_counts(dropna=False)
        print(f"  üìä After cleaning - unique values: {len(after_counts)}")

        print(f"  üìù Final values:")
        for val, count in after_counts.items():
            percentage = (count / len(df)) * 100
            print(f"    '{val}': {count} ({percentage:.1f}%)")

        cleaning_stats[col] = {
            'before_unique': len(before_counts),
            'after_unique': len(after_counts),
            'final_distribution': after_counts.to_dict()
        }

    # Handle missing values in other text columns
    text_columns = ['client_notes', 'currency']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].fillna('').replace('', 'N/A' if col == 'client_notes' else 'USD')

    return df, cleaning_stats

df_clean, text_stats = clean_text_data(df_clean)

# =====================================================
# 4. HANDLE MISSING VALUES
# =====================================================

def handle_missing_values(df):
    """Apply business rules for missing values"""
    print(f"\nüï≥Ô∏è  HANDLING MISSING VALUES")
    print("-" * 30)

    missing_stats = {}

    # Business rules for missing values
    missing_rules = {
        'settlement_date': 'Leave as NULL for DIVIDEND transactions, use trade_date + 2 for others',
        'tax': 'Default to 0.00 for missing values',
        'commission': 'Default to 0.00 for missing values (though unusual)',
        'client_notes': 'Already handled - set to N/A'
    }

    print("üìã Applying missing value rules:")

    # 1. Tax amounts - default to 0
    before_tax_missing = df['tax'].isna().sum()
    df['tax'] = df['tax'].fillna(0.0)
    after_tax_missing = df['tax'].isna().sum()
    print(f"  ‚Ä¢ tax: {before_tax_missing} ‚Üí {after_tax_missing} missing")

    # 2. Commission - default to 0 (unusual but handle gracefully)
    before_comm_missing = df['commission'].isna().sum()
    df['commission'] = df['commission'].fillna(0.0)
    after_comm_missing = df['commission'].isna().sum()
    print(f"  ‚Ä¢ commission: {before_comm_missing} ‚Üí {after_comm_missing} missing")

    # 3. Settlement date - business logic
    before_settle_missing = df['settlement_date'].isna().sum()
    # For now, leave as is - we'll handle this in business rules section
    print(f"  ‚Ä¢ settlement_date: {before_settle_missing} missing (handled by business rules)")

    # 4. Price and quantity - these should not be missing for valid transactions
    critical_missing = {
        'price_per_share': df['price_per_share'].isna().sum(),
        'total_amount': df['total_amount'].isna().sum(),
        'quantity': df['quantity'].isna().sum()
    }

    print(f"  üö® Critical missing values (should investigate):")
    for col, count in critical_missing.items():
        if count > 0:
            print(f"    ‚Ä¢ {col}: {count} missing")

    missing_stats = {
        'tax_filled': before_tax_missing - after_tax_missing,
        'commission_filled': before_comm_missing - after_comm_missing,
        'critical_missing': critical_missing
    }

    return df, missing_stats

df_clean, missing_stats = handle_missing_values(df_clean)

# =====================================================
# 5. DATA TYPE CONVERSIONS
# =====================================================

def convert_data_types(df):
    """Convert columns to appropriate data types"""
    print(f"\nüîÑ CONVERTING DATA TYPES")
    print("-" * 25)

    print("üìã Converting to appropriate types:")

    # Date columns
    date_columns = ['trade_date', 'settlement_date']
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            print(f"  ‚úÖ {col} ‚Üí datetime64")

    # Numeric columns
    numeric_columns = ['price_per_share', 'total_amount', 'commission', 'tax', 'quantity']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            print(f"  ‚úÖ {col} ‚Üí float64")

    # Categorical columns (for memory efficiency)
    categorical_columns = ['transaction_type', 'account_type', 'risk_rating', 'currency', 'market']
    for col in categorical_columns:
        if col in df.columns:
            df[col] = df[col].astype('category')
            print(f"  ‚úÖ {col} ‚Üí category")

    # Show final data types
    print(f"\nüìä Final data types:")
    for col, dtype in df.dtypes.items():
        if not col.endswith('_original'):  # Skip backup columns
            print(f"  {col}: {dtype}")

    return df

df_clean = convert_data_types(df_clean)

# =====================================================
# 6. CLEANING VALIDATION & SUMMARY
# =====================================================

def validate_cleaning_results(df_raw, df_clean):
    """Validate that cleaning was successful"""
    print(f"\n‚úÖ CLEANING VALIDATION")
    print("-" * 25)

    # Compare row counts
    print(f"üìä Row count comparison:")
    print(f"  Raw data: {len(df_raw):,}")
    print(f"  Cleaned data: {len(df_clean):,}")
    print(f"  Rows lost: {len(df_raw) - len(df_clean):,}")

    # Check for remaining data quality issues
    print(f"\nüîç Remaining data quality issues:")

    # Missing values in critical columns
    critical_cols = ['transaction_id', 'trade_date', 'symbol', 'transaction_type', 'quantity', 'price_per_share']
    for col in critical_cols:
        if col in df_clean.columns:
            missing = df_clean[col].isna().sum()
            if missing > 0:
                print(f"  ‚ö†Ô∏è  {col}: {missing} missing values")
            else:
                print(f"  ‚úÖ {col}: No missing values")

    # Date format consistency
    date_cols = ['trade_date', 'settlement_date']
    for col in date_cols:
        if col in df_clean.columns:
            if df_clean[col].dtype == 'datetime64[ns]':
                print(f"  ‚úÖ {col}: Properly formatted as datetime")
            else:
                print(f"  ‚ö†Ô∏è  {col}: Not in datetime format")

    # Numeric format consistency
    numeric_cols = ['price_per_share', 'total_amount', 'commission', 'tax', 'quantity']
    for col in numeric_cols:
        if col in df_clean.columns:
            if pd.api.types.is_numeric_dtype(df_clean[col]):
                print(f"  ‚úÖ {col}: Properly formatted as numeric")
            else:
                print(f"  ‚ö†Ô∏è  {col}: Not in numeric format")

    return True

validation_result = validate_cleaning_results(df_raw, df_clean)

# =====================================================
# 7. SAVE CLEANED DATA
# =====================================================

print(f"\nüíæ SAVING CLEANED DATA")
print("-" * 20)

# Remove backup columns before saving
columns_to_keep = [col for col in df_clean.columns if not col.endswith('_original')]
df_final = df_clean[columns_to_keep].copy()

# Save cleaned data
df_final.to_csv('financial_transactions_cleaned.csv', index=False)
print(f"‚úÖ Saved cleaned data: 'financial_transactions_cleaned.csv'")
print(f"üìä Final dataset: {df_final.shape[0]:,} rows √ó {df_final.shape[1]} columns")

# Show sample of cleaned data
print(f"\nüìã Sample of cleaned data:")
print(df_final.head(3))

print(f"\n" + "="*50)
print(f"üéâ DATA CLEANING COMPLETE!")
print(f"‚úÖ All format inconsistencies resolved")
print(f"‚úÖ Missing values handled appropriately")
print(f"‚úÖ Data types converted correctly")
print(f"‚úÖ Ready for transformation and loading")
print(f"üöÄ Next Step: Business Rules & Calculations")
print(f"="*50)

"""‚öôÔ∏è What This Transformation Step Does:
1. Business Calculations üí∞

Net Amount: total_amount - commission - tax
Settlement Days: Calendar days between trade and settlement
Absolute Amount: For reporting purposes (removes negative signs)
Settlement Status: Boolean flag for settled vs pending

2. Categorization & Classification üè∑Ô∏è

Asset Classes: EQUITY, ETF, CRYPTOCURRENCY, CURRENCY, FIXED_INCOME
Risk Scores: Low=1, Medium=2, High=3, Very High=4
Account Flags: is_retirement_account, is_corporate_account
Transaction Sizes: Small (<$1K), Medium ($1K-$10K), Large ($10K-$100K), Very Large (>$100K)

3. Date Dimension Preparation üìÖ
Extracts components for time-based analysis:

Year, Month, Quarter
Day of week, Day name, Month name
Weekend flag

4. Business Rules Validation ‚úÖ
Checks critical business logic:

BUY transactions should be negative
SELL transactions should be positive
Settlement date ‚â• trade date
Commission amounts are non-negative
Quantities are positive

5. Dimensional Modeling Prep üåü
Identifies unique values for dimension tables:

Unique accounts, customers, securities
Unique brokers, dates
Prepares for star schema creation

üéØ Why Each Transformation Matters:
Business Calculations:

Essential for financial reporting and analysis
Net amount shows true transaction impact
Settlement tracking for operational efficiency

Categorization:

Enables portfolio analysis by asset class
Risk-based reporting for compliance
Account type analysis for customer segmentation

Date Dimensions:

Time-based analytics (monthly trends, quarterly reports)
Seasonal analysis
Business day vs weekend analysis

Business Rules Validation:

Ensures data integrity for financial calculations
Flags potential data quality issues
Maintains audit trail for compliance
"""

# =====================================================
# ETL STEP 3: TRANSFORM & APPLY BUSINESS RULES
# Calculate derived fields and implement business logic
# =====================================================

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("‚öôÔ∏è  ETL STEP 3: TRANSFORM & APPLY BUSINESS RULES")
print("=" * 55)
print("Goal: Calculate derived fields and implement business logic")
print("=" * 55)

# Load cleaned data
df = pd.read_csv('financial_transactions_cleaned.csv')
print(f"‚úÖ Loaded cleaned data: {len(df):,} rows")

# Convert date columns back to datetime (CSV converts them to strings)
df['trade_date'] = pd.to_datetime(df['trade_date'])
df['settlement_date'] = pd.to_datetime(df['settlement_date'])

# =====================================================
# 1. BUSINESS CALCULATIONS
# =====================================================

def apply_business_calculations(df):
    """Apply financial business calculations"""
    print(f"\nüí∞ APPLYING BUSINESS CALCULATIONS")
    print("-" * 35)

    # 1. Calculate net amount (gross - commission - tax)
    print("üî¢ Calculating net_amount...")
    df['net_amount'] = df['total_amount'] - df['commission'] - df['tax']

    # Show examples
    sample_calc = df[['total_amount', 'commission', 'tax', 'net_amount']].head(3)
    print("üìù Examples of net_amount calculation:")
    for idx, row in sample_calc.iterrows():
        print(f"  Row {idx}: {row['total_amount']} - {row['commission']} - {row['tax']} = {row['net_amount']}")

    # 2. Calculate days to settlement
    print(f"\nüìÖ Calculating settlement periods...")

    def calculate_settlement_days(trade_date, settlement_date):
        """Calculate business days between trade and settlement"""
        if pd.isna(settlement_date) or pd.isna(trade_date):
            return np.nan

        # Calculate calendar days difference
        days_diff = (settlement_date - trade_date).days
        return days_diff

    df['days_to_settle'] = df.apply(
        lambda row: calculate_settlement_days(row['trade_date'], row['settlement_date']),
        axis=1
    )

    # Show settlement day statistics
    settlement_stats = df['days_to_settle'].describe()
    print("üìä Settlement days statistics:")
    print(f"  Mean: {settlement_stats['mean']:.1f} days")
    print(f"  Most common: {df['days_to_settle'].mode().iloc[0] if len(df['days_to_settle'].mode()) > 0 else 'N/A'} days")
    print(f"  Range: {settlement_stats['min']:.0f} to {settlement_stats['max']:.0f} days")

    # 3. Calculate absolute amount (for reporting)
    print(f"\nüíµ Calculating absolute amounts...")
    df['absolute_amount'] = df['total_amount'].abs()

    # 4. Determine if transaction is settled
    print(f"\n‚úÖ Determining settlement status...")
    df['is_settled'] = df['settlement_date'].notna()
    settled_count = df['is_settled'].sum()
    print(f"  Settled transactions: {settled_count:,} ({settled_count/len(df)*100:.1f}%)")

    return df

df = apply_business_calculations(df)

# =====================================================
# 2. CATEGORIZATION & CLASSIFICATION
# =====================================================

def apply_categorization_rules(df):
    """Apply business categorization rules"""
    print(f"\nüè∑Ô∏è  APPLYING CATEGORIZATION RULES")
    print("-" * 35)

    # 1. Asset class classification
    print("üîç Classifying asset classes...")

    def classify_asset_class(symbol):
        """Classify securities into asset classes"""
        symbol_str = str(symbol).upper()

        if 'BTC' in symbol_str or 'ETH' in symbol_str:
            return 'CRYPTOCURRENCY'
        elif '/' in symbol_str:  # EUR/USD, GBP/USD
            return 'CURRENCY'
        elif 'BOND' in symbol_str:
            return 'FIXED_INCOME'
        elif symbol_str in ['SPY', 'QQQ', 'VTI', 'GLD', 'SLV']:
            return 'ETF'
        else:
            return 'EQUITY'

    df['asset_class'] = df['symbol'].apply(classify_asset_class)

    # Show asset class distribution
    asset_distribution = df['asset_class'].value_counts()
    print("üìä Asset class distribution:")
    for asset_class, count in asset_distribution.items():
        percentage = (count / len(df)) * 100
        print(f"  {asset_class}: {count:,} ({percentage:.1f}%)")

    # 2. Risk score mapping
    print(f"\n‚ö†Ô∏è  Mapping risk ratings to scores...")

    risk_mapping = {
        'Low': 1,
        'Medium': 2,
        'High': 3,
        'Very High': 4,
        'Unknown': 0
    }

    df['risk_score'] = df['risk_rating'].map(risk_mapping)

    # Show risk distribution
    risk_distribution = df.groupby(['risk_rating', 'risk_score']).size().reset_index(name='count')
    print("üìä Risk rating mapping:")
    for _, row in risk_distribution.iterrows():
        print(f"  '{row['risk_rating']}' ‚Üí Score {row['risk_score']}: {row['count']:,} transactions")

    # 3. Account type flags
    print(f"\nüè¶ Creating account type flags...")

    df['is_retirement_account'] = df['account_type'] == 'Retirement'
    df['is_corporate_account'] = df['account_type'] == 'Corporate'

    retirement_count = df['is_retirement_account'].sum()
    corporate_count = df['is_corporate_account'].sum()
    print(f"  Retirement accounts: {retirement_count:,} transactions")
    print(f"  Corporate accounts: {corporate_count:,} transactions")

    # 4. Transaction size categorization
    print(f"\nüìè Categorizing transaction sizes...")

    def categorize_transaction_size(amount):
        """Categorize transactions by size"""
        abs_amount = abs(amount)
        if abs_amount < 1000:
            return 'Small'
        elif abs_amount < 10000:
            return 'Medium'
        elif abs_amount < 100000:
            return 'Large'
        else:
            return 'Very Large'

    df['transaction_size_category'] = df['total_amount'].apply(categorize_transaction_size)

    size_distribution = df['transaction_size_category'].value_counts()
    print("üìä Transaction size distribution:")
    for size, count in size_distribution.items():
        percentage = (count / len(df)) * 100
        print(f"  {size}: {count:,} ({percentage:.1f}%)")

    return df

df = apply_categorization_rules(df)

# =====================================================
# 3. DATE DIMENSION PREPARATION
# =====================================================

def prepare_date_dimensions(df):
    """Extract date components for dimension tables"""
    print(f"\nüìÖ PREPARING DATE DIMENSIONS")
    print("-" * 30)

    # Extract date components from trade_date
    print("üîç Extracting date components...")

    df['trade_year'] = df['trade_date'].dt.year
    df['trade_month'] = df['trade_date'].dt.month
    df['trade_quarter'] = df['trade_date'].dt.quarter
    df['trade_day_of_week'] = df['trade_date'].dt.dayofweek + 1  # Monday=1
    df['trade_day_name'] = df['trade_date'].dt.day_name()
    df['trade_month_name'] = df['trade_date'].dt.month_name()
    df['trade_is_weekend'] = df['trade_date'].dt.dayofweek >= 5

    # Show date component examples
    date_sample = df[['trade_date', 'trade_year', 'trade_month', 'trade_quarter',
                     'trade_day_name', 'trade_is_weekend']].head(3)
    print("üìù Date component examples:")
    print(date_sample)

    # Date range summary
    date_range = {
        'min_date': df['trade_date'].min(),
        'max_date': df['trade_date'].max(),
        'unique_dates': df['trade_date'].dt.date.nunique()
    }

    print(f"\nüìä Date range summary:")
    print(f"  Date range: {date_range['min_date'].date()} to {date_range['max_date'].date()}")
    print(f"  Unique trading dates: {date_range['unique_dates']}")
    print(f"  Weekend transactions: {df['trade_is_weekend'].sum()} ({df['trade_is_weekend'].mean()*100:.1f}%)")

    return df

df = prepare_date_dimensions(df)

# =====================================================
# 4. DATA VALIDATION & BUSINESS RULES
# =====================================================

def validate_business_rules(df):
    """Validate business rules and flag violations"""
    print(f"\n‚úÖ VALIDATING BUSINESS RULES")
    print("-" * 30)

    violations = {}

    # Rule 1: BUY transactions should have negative total_amount
    print("üîç Rule 1: BUY transactions should be negative...")
    buy_positive = df[(df['transaction_type'] == 'BUY') & (df['total_amount'] > 0)]
    violations['buy_positive'] = len(buy_positive)
    if len(buy_positive) > 0:
        print(f"  ‚ö†Ô∏è  Violation: {len(buy_positive)} BUY transactions with positive amounts")
    else:
        print(f"  ‚úÖ All BUY transactions have negative amounts")

    # Rule 2: SELL transactions should have positive total_amount
    print("üîç Rule 2: SELL transactions should be positive...")
    sell_negative = df[(df['transaction_type'] == 'SELL') & (df['total_amount'] < 0)]
    violations['sell_negative'] = len(sell_negative)
    if len(sell_negative) > 0:
        print(f"  ‚ö†Ô∏è  Violation: {len(sell_negative)} SELL transactions with negative amounts")
    else:
        print(f"  ‚úÖ All SELL transactions have positive amounts")

    # Rule 3: Settlement date should be >= trade date (when both exist)
    print("üîç Rule 3: Settlement date should be >= trade date...")
    invalid_settlements = df[
        (df['settlement_date'].notna()) &
        (df['trade_date'].notna()) &
        (df['settlement_date'] < df['trade_date'])
    ]
    violations['invalid_settlements'] = len(invalid_settlements)
    if len(invalid_settlements) > 0:
        print(f"  ‚ö†Ô∏è  Violation: {len(invalid_settlements)} settlements before trade date")
    else:
        print(f"  ‚úÖ All settlement dates are valid")

    # Rule 4: Commission should be non-negative
    print("üîç Rule 4: Commission should be non-negative...")
    negative_commission = df[df['commission'] < 0]
    violations['negative_commission'] = len(negative_commission)
    if len(negative_commission) > 0:
        print(f"  ‚ö†Ô∏è  Violation: {len(negative_commission)} negative commission amounts")
    else:
        print(f"  ‚úÖ All commission amounts are non-negative")

    # Rule 5: Quantity should be positive
    print("üîç Rule 5: Quantity should be positive...")
    zero_or_negative_qty = df[df['quantity'] <= 0]
    violations['invalid_quantity'] = len(zero_or_negative_qty)
    if len(zero_or_negative_qty) > 0:
        print(f"  ‚ö†Ô∏è  Violation: {len(zero_or_negative_qty)} transactions with zero/negative quantity")
    else:
        print(f"  ‚úÖ All quantities are positive")

    # Summary
    total_violations = sum(violations.values())
    print(f"\nüìä Business rules validation summary:")
    print(f"  Total violations: {total_violations}")
    print(f"  Clean records: {len(df) - total_violations} ({(len(df) - total_violations)/len(df)*100:.1f}%)")

    return violations

business_violations = validate_business_rules(df)

# =====================================================
# 5. PREPARE FOR DIMENSIONAL MODELING
# =====================================================

def prepare_for_dimensional_modeling(df):
    """Prepare data structure for star schema creation"""
    print(f"\nüåü PREPARING FOR DIMENSIONAL MODELING")
    print("-" * 40)

    # Create unique dimension data
    print("üîç Identifying unique dimension values...")

    # Account dimension data
    account_dim_data = df.groupby('account_number').agg({
        'customer_id': 'first',
        'account_type': 'first',
        'is_retirement_account': 'first',
        'is_corporate_account': 'first'
    }).reset_index()

    print(f"  üìä Unique accounts: {len(account_dim_data)}")

    # Customer dimension data
    customer_dim_data = df.groupby('customer_id').agg({
        'risk_rating': 'first',
        'risk_score': 'first'
    }).reset_index()

    print(f"  üìä Unique customers: {len(customer_dim_data)}")

    # Security dimension data
    security_dim_data = df.groupby('symbol').agg({
        'security_name': 'first',
        'asset_class': 'first',
        'market': 'first'
    }).reset_index()

    print(f"  üìä Unique securities: {len(security_dim_data)}")

    # Broker dimension data
    broker_dim_data = df['broker_id'].unique()
    print(f"  üìä Unique brokers: {len(broker_dim_data)}")

    # Date dimension data
    unique_dates = df['trade_date'].dt.date.unique()
    print(f"  üìä Unique dates: {len(unique_dates)}")

    dimension_summary = {
        'accounts': len(account_dim_data),
        'customers': len(customer_dim_data),
        'securities': len(security_dim_data),
        'brokers': len(broker_dim_data),
        'dates': len(unique_dates)
    }

    return dimension_summary

dim_summary = prepare_for_dimensional_modeling(df)

# =====================================================
# 6. FINAL TRANSFORMATION SUMMARY
# =====================================================

print(f"\nüíæ SAVING TRANSFORMED DATA")
print("-" * 25)

# Save the fully transformed data
df.to_csv('financial_transactions_transformed.csv', index=False)
print(f"‚úÖ Saved transformed data: 'financial_transactions_transformed.csv'")

# Show final dataset structure
print(f"\nüìã Final dataset structure:")
print(f"  Rows: {len(df):,}")
print(f"  Columns: {len(df.columns)}")

print(f"\nüìä New calculated fields added:")
calculated_fields = [
    'net_amount', 'days_to_settle', 'absolute_amount', 'is_settled',
    'asset_class', 'risk_score', 'is_retirement_account', 'is_corporate_account',
    'transaction_size_category', 'trade_year', 'trade_month', 'trade_quarter',
    'trade_day_of_week', 'trade_day_name', 'trade_month_name', 'trade_is_weekend'
]

for field in calculated_fields:
    if field in df.columns:
        print(f"  ‚úÖ {field}")

print(f"\nüìù Sample of transformed data:")
sample_cols = ['transaction_id', 'symbol', 'transaction_type', 'total_amount',
               'net_amount', 'asset_class', 'is_settled', 'trade_year']
print(df[sample_cols].head(3))

print(f"\n" + "="*60)
print(f"üéâ TRANSFORMATION COMPLETE!")
print(f"‚úÖ Business calculations applied")
print(f"‚úÖ Categorization rules implemented")
print(f"‚úÖ Date dimensions prepared")
print(f"‚úÖ Business rules validated")
print(f"‚úÖ Ready for dimensional modeling")
print(f"üöÄ Next Step: Create Star Schema (Fact & Dimension Tables)")
print(f"="*60)

# =====================================================
# ETL STEP 4: CREATE STAR SCHEMA - DIMENSIONAL MODELING
# Build fact and dimension tables for data warehouse
# =====================================================

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sqlite3
import warnings
warnings.filterwarnings('ignore')

print("üåü ETL STEP 4: CREATE STAR SCHEMA")
print("=" * 40)
print("Goal: Build fact and dimension tables for analytics")
print("=" * 40)

# Load transformed data
df = pd.read_csv('financial_transactions_transformed.csv')
print(f"‚úÖ Loaded transformed data: {len(df):,} rows")

# Convert date columns back to datetime
df['trade_date'] = pd.to_datetime(df['trade_date'])
df['settlement_date'] = pd.to_datetime(df['settlement_date'])

# =====================================================
# 1. CREATE DIMENSION TABLES
# =====================================================

def create_dimension_tables(df):
    """Create all dimension tables with surrogate keys"""
    print(f"\nüìä CREATING DIMENSION TABLES")
    print("-" * 35)

    dimensions = {}

    # 1. DATE DIMENSION
    print("üóìÔ∏è  Creating date_dimension...")

    # Get all unique dates from the dataset
    all_dates = pd.concat([
        df['trade_date'].dropna(),
        df['settlement_date'].dropna()
    ]).unique()

    # Create comprehensive date dimension
    date_list = []
    for date in sorted(all_dates):
        if pd.notna(date):
            date_obj = pd.to_datetime(date)
            date_list.append({
                'date_value': date_obj.date(),
                'year': date_obj.year,
                'quarter': date_obj.quarter,
                'month': date_obj.month,
                'day': date_obj.day,
                'day_of_week': date_obj.dayofweek + 1,  # Monday = 1
                'day_name': date_obj.day_name(),
                'month_name': date_obj.month_name(),
                'is_weekend': date_obj.dayofweek >= 5,
                'is_holiday': False  # Could be enhanced with holiday calendar
            })

    date_dimension = pd.DataFrame(date_list)
    date_dimension = date_dimension.drop_duplicates('date_value').reset_index(drop=True)
    date_dimension['date_sk'] = range(1, len(date_dimension) + 1)

    # Reorder columns
    date_dimension = date_dimension[['date_sk', 'date_value', 'year', 'quarter', 'month',
                                   'day', 'day_of_week', 'day_name', 'month_name',
                                   'is_weekend', 'is_holiday']]

    print(f"  ‚úÖ Created date_dimension: {len(date_dimension)} unique dates")
    print(f"     Date range: {date_dimension['date_value'].min()} to {date_dimension['date_value'].max()}")

    # 2. CUSTOMER DIMENSION
    print("üë§ Creating customer_dimension...")

    customer_data = df.groupby('customer_id').agg({
        'risk_rating': 'first',
        'risk_score': 'first'
    }).reset_index()

    customer_dimension = customer_data.copy()
    customer_dimension['customer_sk'] = range(1, len(customer_dimension) + 1)
    customer_dimension['is_active'] = True
    customer_dimension['created_date'] = datetime.now().date()
    customer_dimension['updated_date'] = datetime.now().date()

    # Reorder columns
    customer_dimension = customer_dimension[['customer_sk', 'customer_id', 'risk_rating',
                                           'risk_score', 'is_active', 'created_date', 'updated_date']]

    print(f"  ‚úÖ Created customer_dimension: {len(customer_dimension)} unique customers")

    # 3. ACCOUNT DIMENSION
    print("üè¶ Creating account_dimension...")

    account_data = df.groupby('account_number').agg({
        'customer_id': 'first',
        'account_type': 'first',
        'is_retirement_account': 'first',
        'is_corporate_account': 'first'
    }).reset_index()

    account_dimension = account_data.copy()
    account_dimension['account_sk'] = range(1, len(account_dimension) + 1)
    account_dimension['is_active'] = True
    account_dimension['created_date'] = datetime.now().date()
    account_dimension['updated_date'] = datetime.now().date()

    # Reorder columns
    account_dimension = account_dimension[['account_sk', 'account_number', 'customer_id',
                                         'account_type', 'is_retirement_account',
                                         'is_corporate_account', 'is_active',
                                         'created_date', 'updated_date']]

    print(f"  ‚úÖ Created account_dimension: {len(account_dimension)} unique accounts")

    # 4. SECURITY DIMENSION
    print("üìà Creating security_dimension...")

    security_data = df.groupby('symbol').agg({
        'security_name': 'first',
        'asset_class': 'first',
        'market': 'first'
    }).reset_index()

    security_dimension = security_data.copy()
    security_dimension['security_sk'] = range(1, len(security_dimension) + 1)
    security_dimension['is_active'] = True
    security_dimension['created_date'] = datetime.now().date()
    security_dimension['updated_date'] = datetime.now().date()

    # Reorder columns
    security_dimension = security_dimension[['security_sk', 'symbol', 'security_name',
                                           'asset_class', 'market', 'is_active',
                                           'created_date', 'updated_date']]

    print(f"  ‚úÖ Created security_dimension: {len(security_dimension)} unique securities")

    # 5. BROKER DIMENSION
    print("üè¢ Creating broker_dimension...")

    broker_data = df['broker_id'].unique()
    broker_dimension = pd.DataFrame({
        'broker_sk': range(1, len(broker_data) + 1),
        'broker_id': broker_data,
        'broker_name': [f"Broker {bid}" for bid in broker_data],  # Could be enhanced with real names
        'is_active': True,
        'created_date': datetime.now().date(),
        'updated_date': datetime.now().date()
    })

    print(f"  ‚úÖ Created broker_dimension: {len(broker_dimension)} unique brokers")

    # Store all dimensions
    dimensions = {
        'date_dimension': date_dimension,
        'customer_dimension': customer_dimension,
        'account_dimension': account_dimension,
        'security_dimension': security_dimension,
        'broker_dimension': broker_dimension
    }

    return dimensions

dimensions = create_dimension_tables(df)

# =====================================================
# 2. CREATE FACT TABLE
# =====================================================

def create_fact_table(df, dimensions):
    """Create the main fact table with foreign keys to dimensions"""
    print(f"\nüéØ CREATING FACT TABLE")
    print("-" * 25)

    print("üîó Joining with dimension tables to get surrogate keys...")

    # Start with the main transaction data
    fact_table = df.copy()

    # Add surrogate keys by joining with dimension tables

    # 1. Add date surrogate keys
    print("  üìÖ Adding date surrogate keys...")

    # Trade date key
    fact_table['trade_date_value'] = fact_table['trade_date'].dt.date
    fact_table = fact_table.merge(
        dimensions['date_dimension'][['date_sk', 'date_value']],
        left_on='trade_date_value',
        right_on='date_value',
        how='left'
    ).rename(columns={'date_sk': 'trade_date_sk'})

    # Settlement date key (can be null)
    fact_table['settlement_date_value'] = fact_table['settlement_date'].dt.date
    fact_table = fact_table.merge(
        dimensions['date_dimension'][['date_sk', 'date_value']],
        left_on='settlement_date_value',
        right_on='date_value',
        how='left',
        suffixes=('', '_settlement')
    ).rename(columns={'date_sk': 'settlement_date_sk'})

    # 2. Add customer surrogate keys
    print("  üë§ Adding customer surrogate keys...")
    fact_table = fact_table.merge(
        dimensions['customer_dimension'][['customer_sk', 'customer_id']],
        on='customer_id',
        how='left'
    )

    # 3. Add account surrogate keys
    print("  üè¶ Adding account surrogate keys...")
    fact_table = fact_table.merge(
        dimensions['account_dimension'][['account_sk', 'account_number']],
        on='account_number',
        how='left'
    )

    # 4. Add security surrogate keys
    print("  üìà Adding security surrogate keys...")
    fact_table = fact_table.merge(
        dimensions['security_dimension'][['security_sk', 'symbol']],
        on='symbol',
        how='left'
    )

    # 5. Add broker surrogate keys
    print("  üè¢ Adding broker surrogate keys...")
    fact_table = fact_table.merge(
        dimensions['broker_dimension'][['broker_sk', 'broker_id']],
        on='broker_id',
        how='left'
    )

    # Create the final fact table with only necessary columns
    print("üéØ Building final fact table structure...")

    transactions_fact = pd.DataFrame({
        'transaction_sk': range(1, len(fact_table) + 1),
        'transaction_id': fact_table['transaction_id'],
        'account_sk': fact_table['account_sk'],
        'customer_sk': fact_table['customer_sk'],
        'security_sk': fact_table['security_sk'],
        'broker_sk': fact_table['broker_sk'],
        'trade_date_sk': fact_table['trade_date_sk'],
        'settlement_date_sk': fact_table['settlement_date_sk'],
        'transaction_type': fact_table['transaction_type'],
        'quantity': fact_table['quantity'],
        'price_per_share': fact_table['price_per_share'],
        'gross_amount': fact_table['total_amount'],
        'commission_amount': fact_table['commission'],
        'tax_amount': fact_table['tax'],
        'net_amount': fact_table['net_amount'],
        'absolute_amount': fact_table['absolute_amount'],
        'currency_code': fact_table['currency'],
        'is_settled': fact_table['is_settled'],
        'days_to_settle': fact_table['days_to_settle'],
        'asset_class': fact_table['asset_class'],
        'transaction_size_category': fact_table['transaction_size_category'],
        'created_timestamp': datetime.now()
    })

    print(f"  ‚úÖ Created transactions_fact: {len(transactions_fact)} records")

    # Validate foreign key relationships
    print("üîç Validating foreign key relationships...")

    validation_results = {}
    validation_results['missing_trade_date_sk'] = transactions_fact['trade_date_sk'].isna().sum()
    validation_results['missing_customer_sk'] = transactions_fact['customer_sk'].isna().sum()
    validation_results['missing_account_sk'] = transactions_fact['account_sk'].isna().sum()
    validation_results['missing_security_sk'] = transactions_fact['security_sk'].isna().sum()
    validation_results['missing_broker_sk'] = transactions_fact['broker_sk'].isna().sum()

    for key, missing_count in validation_results.items():
        if missing_count > 0:
            print(f"  ‚ö†Ô∏è  {key}: {missing_count} missing references")
        else:
            print(f"  ‚úÖ {key}: All references valid")

    return transactions_fact

transactions_fact = create_fact_table(df, dimensions)

# =====================================================
# 3. SAVE TO DATABASE
# =====================================================

def save_to_database(dimensions, fact_table):
    """Save the star schema to a SQLite database"""
    print(f"\nüíæ SAVING STAR SCHEMA TO DATABASE")
    print("-" * 35)

    # Create SQLite database
    conn = sqlite3.connect('financial_datawarehouse.db')

    print("üóÑÔ∏è  Creating database tables...")

    # Save dimension tables
    for table_name, df in dimensions.items():
        df.to_sql(table_name, conn, if_exists='replace', index=False)
        print(f"  ‚úÖ Saved {table_name}: {len(df)} rows")

    # Save fact table
    fact_table.to_sql('transactions_fact', conn, if_exists='replace', index=False)
    print(f"  ‚úÖ Saved transactions_fact: {len(fact_table)} rows")

    # Create indexes for performance
    print("üîç Creating database indexes...")

    index_commands = [
        "CREATE INDEX idx_transactions_trade_date ON transactions_fact(trade_date_sk)",
        "CREATE INDEX idx_transactions_customer ON transactions_fact(customer_sk)",
        "CREATE INDEX idx_transactions_account ON transactions_fact(account_sk)",
        "CREATE INDEX idx_transactions_security ON transactions_fact(security_sk)",
        "CREATE INDEX idx_transactions_type ON transactions_fact(transaction_type)",
        "CREATE INDEX idx_date_year_month ON date_dimension(year, month)",
        "CREATE INDEX idx_customer_risk ON customer_dimension(risk_rating)",
        "CREATE INDEX idx_security_asset_class ON security_dimension(asset_class)"
    ]

    for cmd in index_commands:
        try:
            conn.execute(cmd)
            print(f"  ‚úÖ Created index")
        except sqlite3.OperationalError as e:
            print(f"  ‚ö†Ô∏è  Index creation warning: {e}")

    conn.commit()
    conn.close()

    print(f"‚úÖ Database saved: 'financial_datawarehouse.db'")

    return True

save_to_database(dimensions, transactions_fact)

# =====================================================
# 4. SAVE TO CSV FILES
# =====================================================

def save_to_csv_files(dimensions, fact_table):
    """Save all tables to individual CSV files"""
    print(f"\nüìÅ SAVING TABLES TO CSV FILES")
    print("-" * 30)

    # Save dimension tables
    for table_name, df in dimensions.items():
        filename = f"{table_name}.csv"
        df.to_csv(filename, index=False)
        print(f"  ‚úÖ Saved {filename}: {len(df)} rows")

    # Save fact table
    transactions_fact.to_csv('transactions_fact.csv', index=False)
    print(f"  ‚úÖ Saved transactions_fact.csv: {len(fact_table)} rows")

    return True

save_to_csv_files(dimensions, transactions_fact)

# =====================================================
# 5. STAR SCHEMA VALIDATION & ANALYTICS
# =====================================================

def validate_star_schema(dimensions, fact_table):
    """Validate the star schema and show sample analytics"""
    print(f"\n‚úÖ STAR SCHEMA VALIDATION")
    print("-" * 30)

    # Basic validation
    print("üìä Schema validation:")
    print(f"  ‚Ä¢ Fact table records: {len(fact_table):,}")
    print(f"  ‚Ä¢ Date dimension: {len(dimensions['date_dimension'])} dates")
    print(f"  ‚Ä¢ Customer dimension: {len(dimensions['customer_dimension'])} customers")
    print(f"  ‚Ä¢ Account dimension: {len(dimensions['account_dimension'])} accounts")
    print(f"  ‚Ä¢ Security dimension: {len(dimensions['security_dimension'])} securities")
    print(f"  ‚Ä¢ Broker dimension: {len(dimensions['broker_dimension'])} brokers")

    # Sample analytics queries
    print(f"\nüìà SAMPLE ANALYTICS:")
    print("-" * 20)

    # 1. Transaction volume by asset class
    print("üí∞ Transaction volume by asset class:")
    volume_by_asset = fact_table.groupby('asset_class')['absolute_amount'].agg(['count', 'sum']).round(2)
    for asset_class, row in volume_by_asset.iterrows():
        print(f"  {asset_class}: {row['count']:,} transactions, ${row['sum']:,.2f} volume")

    # 2. Monthly transaction trends
    print(f"\nüìÖ Monthly transaction summary:")
    monthly_data = fact_table.merge(
        dimensions['date_dimension'][['date_sk', 'year', 'month']],
        left_on='trade_date_sk',
        right_on='date_sk'
    )
    monthly_summary = monthly_data.groupby(['year', 'month']).agg({
        'transaction_sk': 'count',
        'absolute_amount': 'sum'
    }).round(2)

    for (year, month), row in monthly_summary.head().iterrows():
        print(f"  {year}-{month:02d}: {row['transaction_sk']:,} transactions, ${row['absolute_amount']:,.2f}")

    # 3. Risk profile analysis
    print(f"\n‚ö†Ô∏è  Risk profile analysis:")
    risk_analysis = fact_table.merge(
        dimensions['customer_dimension'][['customer_sk', 'risk_rating']],
        on='customer_sk'
    )
    risk_summary = risk_analysis.groupby('risk_rating')['absolute_amount'].agg(['count', 'sum']).round(2)
    for risk_level, row in risk_summary.iterrows():
        print(f"  {risk_level}: {row['count']:,} transactions, ${row['sum']:,.2f} volume")

    return True

validate_star_schema(dimensions, transactions_fact)

# =====================================================
# 6. FINAL SUMMARY
# =====================================================

print(f"\n" + "="*60)
print(f"üéâ STAR SCHEMA CREATION COMPLETE!")
print(f"="*60)

print(f"\nüìã DELIVERABLES CREATED:")
print(f"  üóÑÔ∏è  Database: financial_datawarehouse.db")
print(f"  üìÅ CSV Files:")
print(f"    ‚Ä¢ transactions_fact.csv ({len(transactions_fact):,} rows)")
print(f"    ‚Ä¢ date_dimension.csv ({len(dimensions['date_dimension'])} rows)")
print(f"    ‚Ä¢ customer_dimension.csv ({len(dimensions['customer_dimension'])} rows)")
print(f"    ‚Ä¢ account_dimension.csv ({len(dimensions['account_dimension'])} rows)")
print(f"    ‚Ä¢ security_dimension.csv ({len(dimensions['security_dimension'])} rows)")
print(f"    ‚Ä¢ broker_dimension.csv ({len(dimensions['broker_dimension'])} rows)")

print(f"\nüåü STAR SCHEMA BENEFITS:")
print(f"  ‚úÖ Optimized for analytical queries")
print(f"  ‚úÖ Eliminates data redundancy")
print(f"  ‚úÖ Supports fast aggregations")
print(f"  ‚úÖ Enables flexible reporting")
print(f"  ‚úÖ Maintains data integrity with foreign keys")

print(f"\nüöÄ READY FOR ANALYTICS:")
print(f"  ‚Ä¢ Executive dashboards")
print(f"  ‚Ä¢ Risk management reports")
print(f"  ‚Ä¢ Regulatory compliance")
print(f"  ‚Ä¢ Customer analytics")
print(f"  ‚Ä¢ Portfolio analysis")

print(f"\nüéì ETL PIPELINE COMPLETE!")
print(f"  Extract ‚úÖ ‚Üí Transform ‚úÖ ‚Üí Load ‚úÖ")
print(f"="*60)